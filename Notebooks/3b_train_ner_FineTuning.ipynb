{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7TlpyB45pl4z",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_DRIVE = True\n",
    "if USE_DRIVE:\n",
    "    project_path  = \"drive/MyDrive/EFREI_CAMP/\"\n",
    "    from google.colab import drive\n",
    "    import subprocess\n",
    "    subprocess.call(['pip', 'install', \"git+https://github.com/amtam0/flair.git\"])\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "else:\n",
    "    project_path  = \"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baca30a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import FlairEmbeddings, TokenEmbeddings, StackedEmbeddings, WordEmbeddings\n",
    "from typing import List\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer\n",
    "from flair.data import Sentence\n",
    "from flair.data import Corpus\n",
    "from flair.datasets import ColumnCorpus\n",
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "import torch\n",
    "from torch.optim import Adam, AdamW\n",
    "import flair\n",
    "import os\n",
    "import datetime\n",
    "import shutil\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "flair.set_seed(123) #reproductible dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb30907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory where the data resides\n",
    "timestamp_folder = \"\" #toedit\n",
    "models_folder = \"training/models\"\n",
    "data_folder = os.path.join(project_path, \"training/GT\", timestamp_folder)\n",
    "#directory where to save model\n",
    "flair_trainer_path_cbrt = os.path.join(project_path, models_folder, \"cbrt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d49fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define columns\n",
    "columns = {0 : 'text', 1 : 'ner'}\n",
    "\n",
    "# 1. initializing the corpus\n",
    "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
    "                              train_file = 'train.txt',\n",
    "                              dev_file = \"dev.txt\",\n",
    "                              test_file= \"test.txt\"\n",
    "                             )\n",
    "\n",
    "# 2. what tag do we want to predict\n",
    "tag_type = 'ner'\n",
    "\n",
    "# 3. make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hkmG9T9YMB-9",
   "metadata": {},
   "source": [
    "### Check if Gpu is used, expected cell output : `CUDA`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e4c705",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA\")\n",
    "    flair.device = torch.device('cuda') \n",
    "else:\n",
    "    flair.device = torch.device('cpu') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DFtkyS5wMSRP",
   "metadata": {},
   "source": [
    "## Start Multi Models Training\n",
    "### - Check `#FINETUNE` TAG for some finetunable parameters\n",
    "### - Some links are attached for some tips \n",
    "### - Make sure to check `TransformerWordEmbeddings` and `trainer.train` Docstrings for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91992949",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PAPER https://aclanthology.org/C18-1139.pdf\n",
    "#PAPER https://aclanthology.org/N19-4010.pdf\n",
    "\n",
    "params_use_context = [False] #FINETUNE https://arxiv.org/abs/1903.08855\n",
    "params_subtoken_pooling = [\"first\"] #FINETUNE https://github.com/flairNLP/flair/blob/master/resources/docs/embeddings/TRANSFORMER_EMBEDDINGS.md\n",
    "params_reproject_embeddings = [False] #FINETUNE\n",
    "params_batch_size = [8] #FINETUNE\n",
    "params_pretrained_models = [\"roberta-base\"] #FINETUNE https://huggingface.co/transformers/v2.3.0/pretrained_models.html\n",
    "\n",
    "hidden_size = 256 #FINETUNE\n",
    "EPOCHS = 10 #FINETUNE\n",
    "\n",
    "for param_use_context in params_use_context:\n",
    "    for param_subtoken_pooling in params_subtoken_pooling:\n",
    "        for param_reproject_embeddings in params_reproject_embeddings:\n",
    "            for param_batch_size in params_batch_size:\n",
    "                for params_pretrained_model in params_pretrained_models:\n",
    "                    Tensorboard_dir = \"pr{}_uc{}_sp{}_re{}_bs{}\".format(str(params_pretrained_model[:-3]), str(param_use_context)[:2], str(param_subtoken_pooling),\n",
    "                                              str(param_reproject_embeddings)[0], str(param_batch_size))\n",
    "\n",
    "                    # 4. initialize embeddings\n",
    "                    #TIPS https://github.com/flairNLP/flair/blob/master/resources/docs/embeddings/TRANSFORMER_EMBEDDINGS.md\n",
    "                    embeddings_cbrt_base = TransformerWordEmbeddings(params_pretrained_model,\n",
    "                                                                      subtoken_pooling=param_subtoken_pooling, \n",
    "                                                                      layers='-1', #FINETUNE\n",
    "                                                                      allow_long_sentences=True, #FINETUNE\n",
    "                                                                      fine_tune=True,\n",
    "                                                                    layer_mean=True, #FINETUNE\n",
    "                                                                    use_context=param_use_context) \n",
    "\n",
    "                    # 5. initialize sequence tagger\n",
    "                    tagger_cbrt_base : SequenceTagger = SequenceTagger(hidden_size=hidden_size,\n",
    "                                                                embeddings=embeddings_cbrt_base,\n",
    "                                                                tag_dictionary=tag_dictionary,\n",
    "                                                                tag_type=tag_type,\n",
    "                                                                use_crf=False,\n",
    "                                                                use_rnn=False,\n",
    "                                                                reproject_embeddings=param_reproject_embeddings) \n",
    "\n",
    "                    # 6. train\n",
    "                    trainer : ModelTrainer = ModelTrainer(tagger_cbrt_base, corpus)\n",
    "\n",
    "                    trainer.train(os.path.join(flair_trainer_path_cbrt, Tensorboard_dir),\n",
    "                                  optimizer=AdamW, #FINETUNE\n",
    "                                  use_tensorboard=True,\n",
    "                                  tensorboard_log_dir=os.path.join(project_path, models_folder, \"runs\", Tensorboard_dir),\n",
    "                                  # metrics_for_tensorboard = [(\"macro avg\", 'f1-score')] #default\n",
    "                                  learning_rate=7e-5, #FINETUNE\n",
    "                                  anneal_factor= 0.5, #FINETUNE\n",
    "                                  patience=3, #FINETUNE\n",
    "                                  max_epochs=EPOCHS,\n",
    "                                  mini_batch_size=param_batch_size,\n",
    "                                  min_learning_rate=0.0000001, #FINETUNE\n",
    "                                  train_with_dev=False,\n",
    "                                  checkpoint=False,\n",
    "                                  save_final_model=False,\n",
    "                                  monitor_test=True,\n",
    "                                  embeddings_storage_mode ='gpu')\n",
    "\n",
    "                    # to resume training\n",
    "                    # checkpoint = './FLAIR-ner/{}/cbrt/checkpoint.pt'.format(timestamp_folder)\n",
    "                    # trainerbis = ModelTrainer.load_checkpoint(checkpoint, corpus)                                                 \n",
    "\n",
    "                    # remove cache from gpu/memory\n",
    "                    torch.cuda.empty_cache()\n",
    "                    del trainer, tagger_cbrt_base, embeddings_cbrt_base                                       "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "3b_train_ner_FineTuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
