{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2674c6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_DRIVE = True\n",
    "if USE_DRIVE:\n",
    "    project_path  = \"drive/MyDrive/EFREI_CAMP/\"\n",
    "    from google.colab import drive\n",
    "    import subprocess\n",
    "    subprocess.call(['pip', 'install', \"git+https://github.com/amtam0/flair.git\"])\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "else:\n",
    "    project_path  = \"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbdf6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "from flair.datasets import ColumnCorpus\n",
    "from flair.data import Corpus\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6188f80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'training/models/seq/lstm_ucFa_spfirst_last_reT_bs32_emball_flFalse_lr0.2_layersAll/best-model.pt' #toedit\n",
    "# load the NER tagger\n",
    "tagger = SequenceTagger.load(os.path.join(project_path, model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77731ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_flair(model, TEXT_test):\n",
    "    \"\"\"\n",
    "    Predict NER dict\n",
    "    Args:\n",
    "        - model\n",
    "        - TEXT_test (str)\n",
    "    output:\n",
    "        - result_dict(dict): unformatted dict\n",
    "    \"\"\"\n",
    "    sentence = Sentence(TEXT_test)\n",
    "    # predict the tags\n",
    "    model.predict(sentence)\n",
    "    result_dict = sentence.to_dict(\"ner\")\n",
    "    result_dict[\"raw_text\"] = TEXT_test\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "def doc_to_spans_flair(doc):\n",
    "    \"\"\"\n",
    "    format FLAIR prediction dict\n",
    "    Args:\n",
    "        - doc(dict): formatted dict\n",
    "    output:\n",
    "        - result_dict(dict): formatted dict\n",
    "    \"\"\"\n",
    "    spans = []\n",
    "    scores = []\n",
    "    entities = []\n",
    "    results = []\n",
    "    zipped = []\n",
    "    predictions = doc[\"entities\"]\n",
    "    for prediction in predictions:\n",
    "        if not prediction:\n",
    "            continue\n",
    "        spans.append({\n",
    "            'from_name': 'label',\n",
    "            'to_name': 'text',\n",
    "            'type': 'labels',\n",
    "            'value': {\n",
    "                'start': prediction[\"start_pos\"],\n",
    "                'end': prediction[\"end_pos\"],\n",
    "                'text': prediction[\"text\"],\n",
    "                'labels': [str(prediction[\"labels\"][0]).split()[0]],\n",
    "            }\n",
    "        })\n",
    "        scores.append(float(str(prediction[\"labels\"][0]).split()[1].strip(\"()\")))\n",
    "        entities.append(str(prediction[\"labels\"][0]).split()[0])\n",
    "        results.append(prediction[\"text\"])\n",
    "    final_dict = {#\"spans\":spans,\n",
    "                 \"entities\":entities,\n",
    "                 \"scores\":scores,\n",
    "                 \"result\":results,\n",
    "                 \"zipped\":[list(a) for a in zip(results, entities, scores)],\n",
    "                 \"raw_text\":doc[\"raw_text\"]}\n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bPcxeQrT6cx2",
   "metadata": {},
   "source": [
    "### Unit test your model\n",
    "#### Expected output a dict that starts with `entities` key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903200e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_TEXT = \"3 serie de 20 secondes 5 minutes 30 entre chaque série\"\n",
    "print(doc_to_spans_flair(predict_flair(tagger, Test_TEXT)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Rn6zCQSk28Xc",
   "metadata": {},
   "source": [
    "### If you have less than 16 tags, it means that you didn't label enough data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d1b094",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tagger.tag_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hYrmoSvp3cLL",
   "metadata": {},
   "source": [
    "# Below Blocks to test your final model with the API code before packaging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sOgPmq1VllXb",
   "metadata": {},
   "source": [
    "### Block to test your model on a small Evaluation Dataset (similar code will be used for final evaluation)\n",
    "### Expected output a dataframe with one row showing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6yWL8C3Wlrk8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define columns\n",
    "columns = {0 : 'text', 1 : 'ner'}\n",
    "corpus: Corpus = ColumnCorpus(data_folder=os.path.join(project_path, \"Notebooks\"),\n",
    "                              column_format=columns,\n",
    "                              train_file = \"GT_small.txt\",\n",
    "                             test_file= \"GT_small.txt\")\n",
    "\n",
    "start = time.time()\n",
    "test_results = tagger.evaluate(corpus.test, \"ner\")\n",
    "duration = time.time() - start\n",
    "test_results_dict = test_results.classification_report\n",
    "\n",
    "\n",
    "df = pd.DataFrame(test_results_dict).stack().to_frame().T\n",
    "df.columns = df.columns.swaplevel(0, 1)\n",
    "df.sort_index(axis=1, level=0, inplace=True)\n",
    "# df = pd.DataFrame(df.to_records())\n",
    "df.columns = df.columns.to_series().str.join('_')\n",
    "df[\"latency\"] = duration\n",
    "df[\"team_path\"] = model_path\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l6tmlMSJqMAW",
   "metadata": {},
   "source": [
    "### Block to test your LOCAL API before submiiting your URL to Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hHbiPpG3qN5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {\"model_name\":\"\",\n",
    "     \"text\":\"3 serie de 20 secondes 5 minutes 30 entre chaque série\"}\n",
    "\n",
    "URL = \"https://***.eu.ngrok.io\" #toedit\n",
    "headers = {\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "response_post = requests.post(URL, headers=headers,data=json.dumps(test))\n",
    "response_get = requests.get(URL, headers=headers)\n",
    "print(response_post.content.decode(\"utf-8\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "5_inference_test_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:labelstudioml] *",
   "language": "python",
   "name": "conda-env-labelstudioml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
