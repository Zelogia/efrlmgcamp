{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_DRIVE = True\n",
    "if USE_DRIVE:\n",
    "    project_path  = \"drive/MyDrive/EFREI_CAMP/\"\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "else:\n",
    "    project_path  = \"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from difflib import SequenceMatcher\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit `Notebooks/labelstudio_auth.json` before running cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(project_path, 'Notebooks/labelstudio_auth.json')) as json_file:\n",
    "    labelstudio_auth = json.load(json_file)\n",
    "    print(labelstudio_auth)\n",
    "#labelstudio project config\n",
    "URL = labelstudio_auth[\"URL\"]\n",
    "PROJECT_ID = labelstudio_auth[\"PROJECT_ID\"]\n",
    "TOKEN = labelstudio_auth[\"TOKEN\"]\n",
    "\n",
    "URL_TASKS_GET = \"{}/api/projects/{}/export?exportType=JSON\".format(URL, PROJECT_ID)\n",
    "URL_TASKS_GET = os.path.join(URL,\"api/projects/\",PROJECT_ID,\"export?exportType=JSON\")\n",
    "timestamp_folder = datetime.datetime.now().strftime('%d%m%Y_%H%M')\n",
    "\n",
    "#path used to save data for training\n",
    "training_path = os.path.join(project_path, \"training/data/labeled\", timestamp_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GET_POST_(TOKEN=None, URL=None, Type=\"GET\", Json=None):\n",
    "    \"\"\"\n",
    "    params:\n",
    "        - TOKEN (str): Auth. token\n",
    "        - URL (str)\n",
    "        - Type (str): GET, POST or PATCH\n",
    "    returns:\n",
    "        - response\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "    \"Authorization\": \"Token {}\".format(TOKEN),\n",
    "    }\n",
    "    if Type==\"POST\":\n",
    "        if Json:\n",
    "            response = requests.post(URL, headers=headers, json=Json)\n",
    "        else:\n",
    "            response = requests.post(URL, headers=headers)\n",
    "        return response\n",
    "    elif Type==\"GET\":\n",
    "        response = requests.get(URL, headers=headers)\n",
    "        return response.json()\n",
    "    elif Type==\"PATCH\":\n",
    "        response = requests.patch(URL, headers=headers)\n",
    "        return response\n",
    "\n",
    "def load_data_from_json(data=None):\n",
    "    \"\"\"\n",
    "    Load your labeled data exported using Label Studio in json format\n",
    "    Convert the Label Studio JSON format to Spacy format so that\n",
    "    the same can be fed into Spacy / Flair NER models\n",
    "    \"\"\"\n",
    "    TRAIN_DATA_formulaire = []\n",
    "    for i in range(len(data)):\n",
    "        text = data[i]['data']['text']\n",
    "        entities = []\n",
    "        for t in data[i]['annotations'][0]['result']:\n",
    "            if 'start' in t['value']:\n",
    "                start = t['value']['start']\n",
    "                end = t['value']['end']\n",
    "                ent = t['value']['labels'][0]\n",
    "                entities.append((start, end, ent))\n",
    "        TRAIN_DATA_formulaire.append((text, {'entities': entities}))\n",
    "    return TRAIN_DATA_formulaire\n",
    "\n",
    "def create_df_from_data(TRAIN_DATA=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    TEXT = []\n",
    "    ANNOTATION = []\n",
    "    for LINE_TRAIN_DATA in TRAIN_DATA:\n",
    "        text = LINE_TRAIN_DATA[0]\n",
    "        text_Annotation = []\n",
    "        for entity in LINE_TRAIN_DATA[1][\"entities\"]:\n",
    "            entity_text = text[entity[0]:entity[1]]\n",
    "            entity_value = entity[2]\n",
    "            text_Annotation.append((entity_text, entity_value))\n",
    "        TEXT.append(text)\n",
    "        ANNOTATION.append(text_Annotation)\n",
    "    df = pd.DataFrame({'text': TEXT, 'annotation': ANNOTATION})\n",
    "\n",
    "    return df\n",
    "\n",
    "def matcher(string, pattern):\n",
    "    '''\n",
    "    Return the start and end index of any pattern present in the text.\n",
    "    '''\n",
    "    match_list = []\n",
    "    pattern = pattern.strip()\n",
    "    seqMatch = SequenceMatcher(None, string, pattern, autojunk=False)\n",
    "    match = seqMatch.find_longest_match(0, len(string), 0, len(pattern))\n",
    "    if (match.size == len(pattern)):\n",
    "        start = match.a\n",
    "        end = match.a + match.size\n",
    "        match_tup = (start, end)\n",
    "        string = string.replace(pattern, \"X\" * len(pattern), 1)\n",
    "        match_list.append(match_tup)\n",
    "    return match_list, string\n",
    "\n",
    "def mark_sentence(s, match_list):\n",
    "    '''\n",
    "    Marks all the entities in the sentence as per the BIO scheme. \n",
    "    '''\n",
    "    word_dict = {}\n",
    "    for word in s.split():\n",
    "        word_dict[word] = 'O'\n",
    "    for start, end, e_type in match_list:\n",
    "        temp_str = s[start:end]\n",
    "        tmp_list = temp_str.split()\n",
    "        if len(tmp_list) > 1:\n",
    "            word_dict[tmp_list[0]] = 'B-' + e_type\n",
    "            for w in tmp_list[1:]:\n",
    "                word_dict[w] = 'I-' + e_type\n",
    "        else:\n",
    "            word_dict[temp_str] = 'B-' + e_type\n",
    "    return word_dict\n",
    "\n",
    "\n",
    "def create_data(df, filepath):\n",
    "    '''\n",
    "    The function responsible for the creation of data in the said format.\n",
    "    '''\n",
    "    with open(filepath, 'w') as f:\n",
    "        for text, annotation in zip(df.text, df.annotation):\n",
    "            text_ = text\n",
    "            match_list = []\n",
    "            for i in annotation:\n",
    "                a, text_ = matcher(text, i[0])\n",
    "                match_list.append((a[0][0], a[0][1], i[1]))\n",
    "            d = mark_sentence(text, match_list)\n",
    "            for i in d.keys():\n",
    "                f.writelines(i + ' ' + d[i] + '\\n')\n",
    "            f.writelines('\\n')\n",
    "\n",
    "            \n",
    "def save_json(data=None ,filepath=None):\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected output `['train.json','dev.json','test.json','all.json''all_data.txt','train.txt','dev.txt','test.txt']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_data = GET_POST_(TOKEN=TOKEN, URL=URL_TASKS_GET, Type=\"GET\")\n",
    "\n",
    "# split to train 90% dev 10% test 10%\n",
    "# https://datascience.stackexchange.com/a/15136\n",
    "train_json, test_json = train_test_split(JSON_data, test_size=0.1, random_state=1)\n",
    "train_json, dev_json = train_test_split(train_json, test_size=0.1/0.9, random_state=1)\n",
    "\n",
    "# save files in json and text format\n",
    "if not os.path.exists(training_path):\n",
    "    os.makedirs(training_path)\n",
    "    \n",
    "save_json(data=train_json ,filepath=os.path.join(training_path, 'train.json'))\n",
    "save_json(data=dev_json ,filepath=os.path.join(training_path, 'dev.json'))\n",
    "save_json(data=test_json ,filepath=os.path.join(training_path, 'test.json'))\n",
    "save_json(data=JSON_data ,filepath=os.path.join(training_path, 'all.json'))\n",
    "\n",
    "df = create_df_from_data(load_data_from_json(data=JSON_data))\n",
    "train = create_df_from_data(load_data_from_json(data=train_json))\n",
    "dev = create_df_from_data(load_data_from_json(data=dev_json))\n",
    "test = create_df_from_data(load_data_from_json(data=test_json))\n",
    "\n",
    "create_data(df, os.path.join(training_path, 'all_data.txt'))\n",
    "create_data(train, os.path.join(training_path, 'train.txt'))\n",
    "create_data(dev, os.path.join(training_path, 'dev.txt'))\n",
    "create_data(test, os.path.join(training_path, 'test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(training_path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2_generate_data_from_Labelstudio.ipynb",
   "provenance": []
  },
  "creator": "ds5",
  "customFields": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "tags": [
   "annotation",
   "NER",
   "DELIVERY"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
