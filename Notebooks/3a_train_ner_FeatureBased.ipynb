{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1GTsho9hrFg",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_DRIVE = True\n",
    "if USE_DRIVE:\n",
    "    project_path  = \"drive/MyDrive/EFREI_CAMP/\"\n",
    "    from google.colab import drive\n",
    "    import subprocess\n",
    "    subprocess.call(['pip', 'install', \"git+https://github.com/amtam0/flair.git\"])\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "else:\n",
    "    project_path  = \"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f4380",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import FlairEmbeddings, TokenEmbeddings, StackedEmbeddings, WordEmbeddings\n",
    "from typing import List\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer\n",
    "from flair.data import Sentence\n",
    "from flair.data import Corpus\n",
    "from flair.datasets import ColumnCorpus\n",
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "import torch\n",
    "from torch.optim import Adam, AdamW\n",
    "import flair\n",
    "import os\n",
    "import datetime\n",
    "import shutil\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "import pandas as pd\n",
    "flair.set_seed(123) #reproductible dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f48e65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory where the data resides\n",
    "timestamp_folder = \"\" #toedit\n",
    "models_folder = \"training/models\"\n",
    "data_folder = os.path.join(project_path, \"training/GT\", timestamp_folder)\n",
    "\n",
    "#directory where to save model\n",
    "flair_trainer_path_seq = os.path.join(project_path, models_folder, \"seq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9d4524",
   "metadata": {},
   "outputs": [],
   "source": [
    "flair_trainer_path_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa2b1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define columns\n",
    "columns = {0 : 'text', 1 : 'ner'}\n",
    "\n",
    "# 1. initializing the corpus\n",
    "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
    "                              train_file = 'train.txt',\n",
    "                              dev_file = \"dev.txt\",\n",
    "                             test_file= \"test.txt\"\n",
    "                             )\n",
    "\n",
    "# 2. what tag do we want to predict\n",
    "tag_type = 'ner'\n",
    "\n",
    "# 3. make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "H4bE7kKBim8f",
   "metadata": {},
   "source": [
    "### Check if Gpu is used, expected cell output : `CUDA`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d5f090",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA\")\n",
    "    flair.device = torch.device('cuda') \n",
    "else:\n",
    "    flair.device = torch.device('cpu') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cw9NTqB5sScZ",
   "metadata": {},
   "source": [
    "## Start Multi Models Training\n",
    "### - Check `#FINETUNE` TAG for some finetunable parameters\n",
    "### - Some links are attached for some tips \n",
    "### - Make sure to check `TransformerWordEmbeddings` and `trainer.train` Docstrings for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a170d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PAPER https://aclanthology.org/C18-1139.pdf\n",
    "#PAPER https://aclanthology.org/N19-4010.pdf\n",
    "\n",
    "params_subtoken_pooling = [\"first_last\"] #FINETUNE https://github.com/flairNLP/flair/blob/master/resources/docs/embeddings/TRANSFORMER_EMBEDDINGS.md\n",
    "params_use_context = [False] #FINETUNE https://arxiv.org/pdf/2011.06993.pdf voir cet article pour comprendre use context\n",
    "params_reproject_embeddings = [True] #FINETUNE\n",
    "params_batch_size = [32] #FINETUNE\n",
    "params_embs = [\"all\"] #FINETUNE [\"Glove\", \"Fastext\", \"both\", \"Cbrt\", \"all\"] https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_4_ELMO_BERT_FLAIR_EMBEDDING.md\n",
    "params_lr = [0.2] #FINETUNE factor to anneal learning rate ==> near 1 (no annealing)\n",
    "params_use_flair_Embs = [False] #FINETUNE\n",
    "params_pretrained_models = [\"roberta-base\"] #FINETUNE https://huggingface.co/transformers/v2.3.0/pretrained_models.html\n",
    "\n",
    "hidden_size = 256\n",
    "EPOCH = 50 #FINETUNE\n",
    "\n",
    "for param_use_context in params_use_context:\n",
    "    for param_subtoken_pooling in params_subtoken_pooling:\n",
    "        for param_reproject_embeddings in params_reproject_embeddings:\n",
    "            for param_batch_size in params_batch_size:\n",
    "                for param_embs in params_embs:\n",
    "                    for param_lr in params_lr:\n",
    "                        for params_use_flair_Emb in params_use_flair_Embs:\n",
    "                            for params_pretrained_model in params_pretrained_models:\n",
    "                       \n",
    "                                Tensorboard_dir = \"lstm_uc{}_sp{}_re{}_bs{}_emb{}_fl{}_lr{}_layersAll\".format(str(param_use_context)[:2], str(param_subtoken_pooling),\n",
    "                                                            str(param_reproject_embeddings)[0], str(param_batch_size),\n",
    "                                                            str(param_embs)[:], str(params_use_flair_Emb)[:],str(param_lr))\n",
    "\n",
    "                                # 4. initialize embeddings\n",
    "\n",
    "                                embeddings_cbrt_base = TransformerWordEmbeddings(params_pretrained_model,\n",
    "                                                                                subtoken_pooling=param_subtoken_pooling, \n",
    "                                                                                allow_long_sentences=True, #FINETUNE\n",
    "                                                                                layer_mean=True, #FINETUNE\n",
    "                                                                                use_context=param_use_context)\n",
    "                                if params_use_flair_Emb:\n",
    "                                    embedding_types = [\n",
    "                                    FlairEmbeddings('fr-forward'),\n",
    "                                    FlairEmbeddings('fr-backward'),\n",
    "                                ]\n",
    "                                else:\n",
    "                                    embedding_types = []\n",
    "                                \n",
    "                                if param_embs==\"Glove\":\n",
    "                                    embedding_types.insert(0, WordEmbeddings('glove'))\n",
    "                                elif param_embs==\"Fastext\":\n",
    "                                    embedding_types.insert(0, WordEmbeddings('fr'))\n",
    "                                elif param_embs==\"both\":\n",
    "                                    embedding_types = [WordEmbeddings('glove'), WordEmbeddings('fr')] + embedding_types\n",
    "                                elif param_embs==\"Cbrt\":\n",
    "                                    embedding_types.insert(0, embeddings_cbrt_base)\n",
    "                                elif param_embs==\"all\":\n",
    "                                    embedding_types = [WordEmbeddings('glove'), WordEmbeddings('fr'), embeddings_cbrt_base] + embedding_types\n",
    "                                embeddings_seq: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "\n",
    "                                # 5. initialize sequence tagger\n",
    "\n",
    "                                tagger_seq : SequenceTagger = SequenceTagger(hidden_size=hidden_size,\n",
    "                                                        embeddings=embeddings_seq,\n",
    "                                                        tag_dictionary=tag_dictionary,\n",
    "                                                        tag_type=tag_type,\n",
    "                                                        use_crf=True,\n",
    "                                                            reproject_embeddings=param_reproject_embeddings)\n",
    "\n",
    "                                # 6. train   \n",
    "                                trainer : ModelTrainer = ModelTrainer(tagger_seq, corpus)\n",
    "                                trainer.train(os.path.join(flair_trainer_path_seq, Tensorboard_dir),\n",
    "                                            use_tensorboard=True,\n",
    "                                            tensorboard_log_dir=os.path.join(project_path, models_folder, \"runs\", Tensorboard_dir),\n",
    "                                            learning_rate=0.1, #FINETUNE\n",
    "                                            anneal_factor=param_lr,\n",
    "                                            patience=3, #FINETUNE\n",
    "                                            mini_batch_size=param_batch_size,\n",
    "                                            min_learning_rate=0.00001, #FINETUNE\n",
    "                                            max_epochs=EPOCH,\n",
    "                                            train_with_dev=False,#FINETUNE\n",
    "                                            checkpoint=False,\n",
    "                                            save_final_model=False,\n",
    "                                            monitor_test=True, #FINETUNE\n",
    "                                            embeddings_storage_mode ='gpu')\n",
    "\n",
    "                #                 checkpoint = os.path.join(flair_trainer_path_seq,\"checkpoint.pt\")\n",
    "                #                 trainerbis = ModelTrainer.load_checkpoint(checkpoint, corpus)\n",
    "\n",
    "                                # remove cache from gpu/memory\n",
    "                                torch.cuda.empty_cache()\n",
    "                                del trainer, tagger_seq, embeddings_seq, embedding_types                         "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "3a_train_ner_FeatureBased.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
